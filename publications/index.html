<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>publications | Nishad Singhi</title> <meta name="author" content="Nishad Singhi"> <meta name="description" content="Please visit my &lt;a href='https://scholar.google.com/citations?user=L1b6JqsAAAAJ&amp;hl=en' style='text-decoration: underline;'&gt;Google Scholar profile&lt;/a&gt; for a more up-to-date list."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://nishadsinghi.github.io/publications/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Nishad Singhi</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">Please visit my <a href="https://scholar.google.com/citations?user=L1b6JqsAAAAJ&amp;hl=en" style="text-decoration: underline;" rel="external nofollow noopener" target="_blank">Google Scholar profile</a> for a more up-to-date list.</p> </header> <article> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Singhi_2025_Preprint" class="col-sm-8"> <div class="title">When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning</div> <div class="author"> Nishad Singhi, <a href="https://scholar.google.com/citations?user=gAKTYtoAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Hritik Bansal</a>, <a href="https://scholar.google.ca/citations?user=MV7LPnEAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Arian Hosseini</a>, <a href="https://scholar.google.com/citations?hl=en&amp;user=oOhnPUgAAAAJ" rel="external nofollow noopener" target="_blank">Aditya Grover</a>, <a href="https://scholar.google.com/citations?hl=en&amp;user=fqDBtzYAAAAJ" rel="external nofollow noopener" target="_blank">Kai-Wei Chang</a>, Marcus Rohrbach, and Anna Rohrbach</div> <div class="periodical"> <em>In Under Review</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Test-time compute scaling has pushed the boundaries of reasoning with large language models (LLMs), primarily through self-consistency (SC), which takes a majority vote over multiple solutions, or Best-of-N selection, where a verifier chooses the best answer. Recently, generative reward models (GenRMs), using next-token prediction for verification, have emerged as an alternative to SC. A key trade-off arises: should we generate multiple solutions and vote (SC) or invest in sophisticated verifications to select the best from a smaller set? To address this, we train and evaluate several GenRMs against SC under a fixed inference budget (FLOPs). Surprisingly, across diverse models and datasets, we find SC to be more compute-efficient for most practical budgets, while GenRM requires significantly more compute to outperform SC. We further derive inference scaling laws for GenRMs, showing that compute-optimal inference favors scaling solution generation over increasing verifications. This suggests that current GenRMs, despite their promise, are less compute-efficient than expected, underscoring the need to reduce their inference overhead. Ultimately, our work provides practical guidance for optimizing test-time scaling and balancing solution generation with verification to build more efficient inference strategies.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Binz_2024_Preprint" class="col-sm-8"> <div class="title">Centaur: a foundation model of human cognition</div> <div class="author"> Marcel Binz,  ..., Nishad Singhi,  ..., and Eric Schulz</div> <div class="periodical"> <em>In Preprint</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2410.20268" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>Establishing a unified theory of cognition has been a major goal of psychology. While there have been previous attempts to instantiate such theories by building computational models, we currently do not have one model that captures the human mind in its entirety. Here we introduce Centaur, a computational model that can predict and simulate human behavior in any experiment expressible in natural language. We derived Centaur by finetuning a state-of-the-art language model on a novel, large-scale data set called Psych-101. Psych-101 reaches an unprecedented scale, covering trial-by-trial data from over 60,000 participants performing over 10,000,000 choices in 160 experiments. Centaur not only captures the behavior of held-out participants better than existing cognitive models, but also generalizes to new cover stories, structural task modifications, and entirely new domains. Furthermore, we find that the model’s internal representations become more aligned with human neural activity after finetuning. Taken together, Centaur is the first real candidate for a unified model of human cognition. We anticipate that it will have a disruptive impact on the cognitive sciences, challenging the existing paradigm for developing computational models.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Singhi_2024_ECCV" class="col-sm-8"> <div class="title">Improving Intervention Efficacy via Concept Realignment in Concept Bottleneck Models</div> <div class="author"> Nishad Singhi, <a href="https://scholar.google.com/citations?hl=de&amp;user=93ZjIs0AAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Karsten Roth</a>, <a href="https://scholar.google.com/citations?user=eP6FHFAAAAAJ&amp;hl=ko" rel="external nofollow noopener" target="_blank">Jae Myung Kim</a>, and <a href="https://scholar.google.com/citations?user=jQl9RtkAAAAJ&amp;hl=ko" rel="external nofollow noopener" target="_blank">Zeynep Akata</a> </div> <div class="periodical"> <em>In European Conference on Computer Vision (ECCV)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2405.01531" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>Concept Bottleneck Models (CBMs) are designed to ground image classification on human-understandable concepts, to make model decisions interpretable. Crucially, the CBM design also allows for interventions, giving users the ability to modify internal concept choices to intuitively influence the decision behavior of the model. However, existing approaches often require numerous human interventions per image to achieve strong performances, posing practical challenges in scenarios where obtaining human inputs is expensive. This is primarily due to the independent treatment of concepts during intervention, where a change of one concept does not influence the use of other ones in the model’s final decision. To address this issue, we propose a simple concept correction technique to automatically realign concept assignments post-intervention by exploiting statistical relationships between them. In doing so, our approach improves intervention efficacy and raises both classification and concept prediction accuracy across various architectures and real-world datasets. In addition, it easily integrates into existing concept-based architectures without requiring changes to the models themselves. We anticipate that our method will reduce the cost of human-model collaboration, and enhance the feasibility of CBMs in resource-constrained environments.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Oral (Top 2%)</abbr></div> <div id="Bansal_2023_ICCV" class="col-sm-8"> <div class="title">CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning</div> <div class="author"> <a href="https://scholar.google.com/citations?user=gAKTYtoAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Hritik Bansal*</a>, Nishad Singhi*, <a href="https://scholar.google.com/citations?hl=en&amp;user=KK6Yj4IAAAAJ" rel="external nofollow noopener" target="_blank">Yu Yang</a>, <a href="https://scholar.google.com/citations?hl=en&amp;user=klShdV0AAAAJ" rel="external nofollow noopener" target="_blank">Fan Yin</a>, <a href="https://scholar.google.com/citations?hl=en&amp;user=oOhnPUgAAAAJ" rel="external nofollow noopener" target="_blank">Aditya Grover</a>, and <a href="https://scholar.google.com/citations?hl=en&amp;user=fqDBtzYAAAAJ" rel="external nofollow noopener" target="_blank">Kai-Wei Chang</a> </div> <div class="periodical"> <em>In International Conference on Computer Vision (ICCV)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2303.03323" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/nishadsinghi/CleanCLIP" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Multimodal contrastive pretraining has been utilized to train multimodal representation models, like CLIP, on vast amounts of paired image-text data. However, previous studies have highlighted the susceptibility of such models to backdoor attacks. Specifically, when training on backdoored examples, CLIP learns spurious correlations between the embedded backdoor trigger and the target label, aligning their representations in the joint embedding space. With injecting only a few poisoned examples e.g., 75 examples in the 3M pretraining data, the model’s behavior can be significantly manipulated, thus making it hard to detect or unlearn such correlations. To address this issue, we propose CleanCLIP, a finetuning framework that weakens the learned spurious associations introduced by backdoor attacks by re-aligning the representations for individual modalities independently. CleanCLIP can be employed for both unsupervised finetuning on paired image-text data and for supervised finetuning on labeled image data. We demonstrate that unsupervised finetuning with a combination of multimodal contrastive and unimodal self-supervised objectives for individual modalities can significantly reduce the impact of the backdoor attack. Additionally, supervised finetuning on task-specific labeled data of the individual modality, such as image data, removes the backdoor trigger from the CLIP vision encoder. Empirically, we show that CleanCLIP maintains model performance on benign examples while mitigating the impact of a range of backdoor attacks on multimodal contrastive learning.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Oral</abbr></div> <div id="singhi2023toward" class="col-sm-8"> <div class="title">Toward a normative theory of (self-) management by goal-setting</div> <div class="author"> Nishad Singhi, <a href="https://scholar.google.com/citations?hl=en&amp;user=BVbGbgwAAAAJ" rel="external nofollow noopener" target="_blank">Florian Mohnert</a>, <a href="https://scholar.google.com/citations?hl=en&amp;user=Ns8jBNsAAAAJ" rel="external nofollow noopener" target="_blank">Ben Prystawski</a>, and <a href="https://scholar.google.com/citations?hl=en&amp;user=JscQvlUAAAAJ" rel="external nofollow noopener" target="_blank">Falk Lieder</a> </div> <div class="periodical"> <em>In Proceedings of the Annual Meeting of the Cognitive Science Society (CogSci)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2302.02633" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>People are often confronted with problems whose complexity exceeds their cognitive capacities. To deal with this complexity, individuals and managers can break complex problems down into a series of subgoals. Which subgoals are most effective depends on people’s cognitive constraints and the cognitive mechanisms of goal pursuit. This creates an untapped opportunity to derive practical recommendations for which subgoals managers and individuals should set from cognitive models of bounded rationality. To seize this opportunity, we apply the principle of resource-rationality to formulate a mathematically precise normative theory of (self-)management by goal-setting. We leverage this theory to computationally derive optimal subgoals from a resource-rational model of human goal pursuit. Finally, we show that the resulting subgoals improve the problem-solving performance of bounded agents and human participants. This constitutes a first step towards grounding prescriptive theories of management and practical recommendations for goal-setting in computational models of the relevant psychological processes and cognitive limitations.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Full Paper</abbr></div> <div id="singhi2023using" class="col-sm-8"> <div class="title">Using Computational Models to Understand the Role and Nature of Valuation Bias in Mixed Gambles</div> <div class="author"> Nishad Singhi, <a href="https://scholar.google.com/citations?hl=en&amp;user=vEsSCZsAAAAJ" rel="external nofollow noopener" target="_blank">Sumeet Agarwal</a>, and <a href="https://scholar.google.com/citations?hl=en&amp;user=gsrGTEQAAAAJ" rel="external nofollow noopener" target="_blank">Sumitava Mukherjee</a> </div> <div class="periodical"> <em>In Proceedings of the Annual Meeting of the Cognitive Science Society (CogSci)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://escholarship.org/content/qt8p78p112/qt8p78p112.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>It is a well-known observation that people tend to dislike risky situations that could potentially lead to a loss, a phenomenon that is called loss aversion. This is often explained using valuation bias, i.e., the subjective value of losses is larger than the subjective value of gains of equal magnitude. However, recent studies using the drift-diffusion model have shown that a pre-valuation bias towards rejection is also a primary determinant of loss-averse behavior. It has large contributions to model fits, predicts a key relationship between rejection rates and response times, and explains the most individual heterogeneity in the rejection rates of participants. We analyzed data from three previously published experiments using the drift-diffusion model and found that these findings generalize to them. However, we found that valuation bias plays the most important role in predicting how likely a person is to accept a given gamble. Our findings also showed that a person’s loss aversion parameter, which captures their propensity to avoid losses is closely related to valuation bias. These results combined highlight the importance of valuation bias in understanding people’s choice patterns. Finally, using the leaky, competing accumulator model, we show strong mimicking between valuation bias and an attentional bias wherein people pay more attention to losses as compared to gains. This finding suggests that behaviors that seem to arise due to valuation bias may arise due to such an attentional bias.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="singhi2023fMRI" class="col-sm-8"> <div class="title">Motivated With Joy or Anxiety: Does Approach-Avoidance Goal Framing Elicit Differential Reward-Network Activation In The Brain?</div> <div class="author"> Nishad Singhi, <a href="https://scholar.google.co.jp/citations?user=_mKkd00AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Michiko Sakaki</a>, <a href="https://scholar.google.com/citations?user=r0AJphIAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Kou Murayama</a>, Madoka Matsumoto, Keise Izuma, Yukihito Yomogida, Ayaka Sugiura, Ryuta Aoki, and Kenji Matsumoto</div> <div class="periodical"> <em>In Psychologie und Gehirn</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://drive.google.com/file/d/1KH6NVtec8VZ-Ih1ALupgekJ5l3tTsqUb/view?usp=sharing" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://drive.google.com/file/d/1EXm_00GahW9R5xUy9dYqABqVQTWApOBD/view?usp=sharing" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Poster</a> </div> <div class="abstract hidden"> <p>There have been a considerable number of behavioral studies showing that approach goals (i.e., achieving success) and avoidance goals (i.e., avoiding failure) lead to different motivational states. Approach goals are associated with positive emotional outcomes, whereas avoidance goals tend to elicit negative emotional states such as anxiety. In this study, we investigated the neural correlates of goal-directed behavior under these goals using fMRI with a game-like, intrinsically motivating task. We especially focused on the key regions implicated in previous work, i.e., the striatum, midbrain, lateral prefrontal cortex, and ventromedial prefrontal cortex. Our findings indicate that despite the fact that approach and avoidance goals produce different motivational states, the striatum, and other key areas are insensitive to the goals. For example, the striatum is activated after a successful outcome in both approach and avoidance goal conditions. These findings suggest that the striatum may encode general motivation or effort mobilization, not the positive motivational state such as intrinsic motivation. Furthermore, we found that the hippocampus was more activated after successful feedback in the approach condition and after failure feedback in the avoidance condition, which suggests that it encodes salient events.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Nishad Singhi. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: March 25, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>